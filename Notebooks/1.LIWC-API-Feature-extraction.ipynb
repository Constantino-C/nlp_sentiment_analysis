{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial LIWC API (Receptiviti API) call to extract features\n",
    "This notebook is run only initially to call the Receptiviti API and extract the vocabulary features which is basically the linguistic analysis on the input data/posts.\n",
    "The Receptiviti API accepts the text for analyis and returns a plethora of semantic features from the text with their scoring. The scores for each feature reveal social and psychological insights, including among others syntactic characteristics of the text. The LIWC is a valuable tool in NLP and brings together the research outcomes of a series of studies in psychology, linguistics and sociology. <p> \n",
    "\n",
    "More information about how the LIWC is set up under the hood and tips on how to call the Receptiviti API can be found on the [official webpage](http://liwc.wpengine.com/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "<br/>\n",
    "\n",
    "* [Import modules](#Import-modules)\n",
    "* [Declaration of functions](#Declaration-of-functions)\n",
    "* [Call the LIWC API - in batch calls](#Call-the-LIWC-API---in-batch-calls) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wordcloud\n",
    "from scipy.stats import pearsonr\n",
    "import string\n",
    "import spacy\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# sp = spacy.load('en_core_web_sm')\n",
    "# stopwords = nltk.corpus.stopwords.words('english')\n",
    "# %config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LIWC Credentials. Sign up for the free version of the API or go for the proprietary one.\n",
    "#Following registration, define here the received key and secret once to be used upon calling the API.\n",
    "# Access the API dashboard to monitor your usage and activity here https://dashboard.receptiviti.com/\n",
    "API_key = #enter details here\n",
    "API_secret = #enter details here\n",
    "API_URL = #enter details here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Declaration of functions\n",
    "\n",
    "The functions _get_payload_and_url()_ and _call_receptiviti_api()_ to be called only when the api is called on new data. Use these two functions with caution as they use up the API allowance.<p>\n",
    "\n",
    "The LIWC API has been created by [Receptiviti](https://www.receptiviti.com/liwc). Over the years, as research advanced the dictionary has been updated with more features; some these are named with the prefix LIWC and some others are named Receptiviti. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check type of content \n",
    "def get_payload_and_url(plot, API_URL):\n",
    "    if len(plot)<1:\n",
    "        print(\"ERROR: 'text' should not be empty\")\n",
    "        return {}\n",
    "    if isinstance(plot, str):\n",
    "        return ({\n",
    "                    \"content\": plot\n",
    "                }, API_URL)\n",
    "    if isinstance(plot, list):\n",
    "        return ([{\n",
    "                    \"content\": content\n",
    "        } for content in plot], API_URL + '/bulk')\n",
    "\n",
    "#call the LIWC vocabulary API to analyse the data\n",
    "def call_receptiviti_api(plot, API_URL, API_key, API_secret):\n",
    "    payload, url = get_payload_and_url(plot, API_URL)\n",
    "    results = []\n",
    "    if len(payload)>0:\n",
    "        response = requests.post(url, data=json.dumps(payload), auth=(API_key, API_secret), headers = {'Content-Type': 'application/json'})\n",
    "        if response.status_code==200:\n",
    "            results = response.json()\n",
    "    return results\n",
    "\n",
    "\n",
    "# filter, tokenize and lemmatize each post\n",
    "def lemmatize_text(posts_corpus):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text=[]\n",
    "    for post, label in zip(posts_corpus.post, posts_corpus.label):\n",
    "        tokens = [word.lower() for word in nltk.word_tokenize(post)]\n",
    "        word_tokens = []\n",
    "        for token in tokens:\n",
    "            # include only words\n",
    "            if re.search('^[a-zA-Z]+$', token):\n",
    "                word_tokens.append(lemmatizer.lemmatize(token))\n",
    "        text.append(\" \".join(word_tokens))\n",
    "    return text\n",
    "\n",
    "#return all LIWC metrics\n",
    "def LIWC_metrics_all(response):\n",
    "    return response['results'][0]['dictionary_measures']\n",
    "\n",
    "#return all Reciptiviti metrics\n",
    "def Rec_metrics_all(response):\n",
    "    return response['results'][0]['receptiviti_measures']\n",
    "\n",
    "\n",
    "#return the n top LIWC metrics\n",
    "def LIWC_metrics_n(response, n=20):\n",
    "    \"\"\"\n",
    "    Select n number of top metrics, sorted highest to lowest  \n",
    "    \"\"\"\n",
    "    #sort the LIWC metrics by higher to lower scores\n",
    "    LIWC_metrics_all= LIWC_metrics(response)\n",
    "    LIW_metrics_sorted = dict(sorted(LIWC_metrics_all.items(), key=lambda item: item[1], reverse=True))\n",
    "    return dict(list(LIW_metrics_sorted.items())[:n])\n",
    "\n",
    "#return the n top Receptiviti metrics\n",
    "def Rec_metrics_n(response, n=20):\n",
    "    \"\"\"\n",
    "    Select n number of top metrics, sorted highest to lowest  \n",
    "    \"\"\"\n",
    "    #sort the receptiviti metrics by higher to lower scores\n",
    "    Rec_metrics_all= Rec_metrics(response)\n",
    "    Rec_metrics_sorted = dict(sorted(Rec_metrics_all.items(), key=lambda item: item[1], reverse=True))\n",
    "    return dict(list(Rec_metrics_sorted.items())[:n])\n",
    "\n",
    "#funtion to create the LIWC_vocabulary analysis dataframe\n",
    "def LIWC_analysis_df(features, posts_sample, LIWC_analysis):\n",
    "    Analysis_scores=np.empty( (len(posts_sample),len(features)) ) \n",
    "    for i, entry in enumerate(LIWC_analysis['results']):\n",
    "        Analysis_scores[i] = list(entry['dictionary_measures'].values()) + list(entry['receptiviti_measures'].values())\n",
    "    scores = pd.DataFrame(data=Analysis_scores, columns=features) \n",
    "    return( pd.concat( [posts_sample, scores], axis=1 ) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare the paths to raw data\n",
    "#Download and extract the dreaddit-dataset in the data/raw directory\n",
    "data_path = os.path.join(\"..\", \"data\", \"raw\", \"dreaddit-train.csv\")\n",
    "features_path = os.path.join(\"..\", \"data\", \"interim\", \"features.csv\")\n",
    "processed_vocab = os.path.join(\"..\", \"data\", \"interim\", \"LIWC-analysis.csv\")\n",
    "#load the raw data\n",
    "df = pd.read_csv(data_path)\n",
    "posts= df[['text','label']]\n",
    "posts.columns = ['post', 'label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a lemmatized dataframe \n",
    "text = lemmatize_text(posts)\n",
    "lemmatized_data = pd.DataFrame([text]).T\n",
    "lemmatized_data.rename(columns={0:'post'},inplace=True)\n",
    "lemmatized_data['label']=posts.label\n",
    "lemmatized_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explaining the measures of the API\n",
    "# https://dashboard.receptiviti.com/docs/frameworks-and-measures/#per-measures\n",
    "# results are more reliable when the word count in each post is >350\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call the LIWC API - in batch calls "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CAUTION!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract all features from the LIWC API and save them in the CSV file for reference. **Used only once at the initial stage.** <p>\n",
    "Break down the posts and run the API in batches, multiple times. **The reason is that the LIWC API cannot acccept a list of posts >1000 objects.** <p>\n",
    "LIWC_analysis = call_receptiviti_api(posts_sample.post.tolist(), API_URL, API_key, API_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  example for sampling the raw data\n",
    "posts_sample=lemmatized_data.iloc[0:999,:]\n",
    "posts_sample=posts_sample.reset_index(drop=True)\n",
    "posts_sample.head()\n",
    "\n",
    "LIWC_analysis = call_receptiviti_api(posts_sample.post.tolist(), API_URL, API_key, API_secret )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section saves all features to a features CSV, for which we define the filepath in our working directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(LIWC_metrics_all(single_result).keys()) + len(Rec_metrics_all(single_result).keys())\n",
    "# = 163 keys in total (116 LIWC measures, 47 Rec measures)\n",
    "\n",
    "\n",
    "#save features to CSV file- used only once\n",
    "Rec_features = pd.DataFrame(Rec_metrics_all(single_result).keys())\n",
    "Rec_features = Rec_features.apply(lambda k: \"Rec_\"+k)\n",
    "LIWC_features = pd.DataFrame(LIWC_metrics_all(single_result).keys())\n",
    "LIWC_features = LIWC_features.apply(lambda k: \"LIWC_\"+k, )\n",
    "all_features = pd.concat([LIWC_features, Rec_features])\n",
    "\n",
    "# set the features path as the path to your working data storage files - data/interim \n",
    "all_features.to_csv(features_path, index=False,header=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section saves the LIWC analysis from the API, in batches in the processed_vocab filepath. This filepath needs to be set in the working directory under data/interim.<p>\n",
    "The recommended way is to manually combine the batch processed data into a common CSV file. An easy way to do this is to name the file path each time as _\"file_1\"_, _\"file_2\"_ and so on until we have passed all data into the API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe to hold the analysis data\n",
    "LIWC_vocabulary_analysis = LIWC_analysis_df(features, posts_sample, LIWC_analysis)\n",
    "LIWC_vocabulary_analysis.head(2)\n",
    "\n",
    "#save the batch of the LIWC anaylsis to CSV file\n",
    "LIWC_vocabulary_analysis.to_csv(processed_vocab, index=False,header=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KidzAbout",
   "language": "python",
   "name": "kidzabout"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
